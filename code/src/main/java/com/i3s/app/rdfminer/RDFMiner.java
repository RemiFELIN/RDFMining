/**
 * 
 */
package com.i3s.app.rdfminer;

import java.io.BufferedReader;
import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;

import org.apache.jena.shared.JenaException;
import org.apache.jena.sparql.engine.http.QueryExceptionHTTP;
import org.apache.log4j.Logger;
import org.apache.log4j.PropertyConfigurator;
import org.json.JSONObject;
import org.kohsuke.args4j.CmdLineException;
import org.kohsuke.args4j.CmdLineParser;

import com.i3s.app.rdfminer.axiom.Axiom;
import com.i3s.app.rdfminer.axiom.AxiomFactory;
import com.i3s.app.rdfminer.axiom.AxiomGenerator;
import com.i3s.app.rdfminer.axiom.CandidateAxiomGenerator;
import com.i3s.app.rdfminer.axiom.IncreasingTimePredictorAxiomGenerator;
import com.i3s.app.rdfminer.axiom.RandomAxiomGenerator;
import com.i3s.app.rdfminer.axiom.type.SubClassOfAxiom;
import com.i3s.app.rdfminer.output.AxiomTestJSON;
import com.i3s.app.rdfminer.parameters.CmdLineParameters;
import com.i3s.app.rdfminer.sparql.SparqlEndpoint;
import Individuals.Phenotype;

//import com.hp.hpl.jena.shared.JenaException;
//import com.hp.hpl.jena.sparql.engine.http.QueryExceptionHTTP;

/**
 * The main class of the RDFMiner experimental tool.
 * <p>
 * More information about OWL 2 may be found in the
 * <a href="http://www.w3.org/TR/2012/REC-owl2-quick-reference-20121211/">OWL2
 * Quick Reference, 2nd Edition</a>.
 * </p>
 * 
 * @author Andrea G. B. Tettamanzi
 *
 */
public class RDFMiner {

	private static Logger logger = Logger.getLogger(RDFMiner.class.getName());

	public static CmdLineParameters parameters = new CmdLineParameters();

	final private static String PREFIXES = "PREFIX owl: <http://www.w3.org/2002/07/owl#>\n"
			+ "PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n"
			+ "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n"
			+ "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n"
			+ "PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n" + "PREFIX dc: <http://purl.org/dc/elements/1.1/>\n"
			+ "PREFIX : <http://dbpedia.org/resource/>\n" + "PREFIX dbpedia2: <http://dbpedia.org/property/>\n"
			+ "PREFIX dbpedia: <http://dbpedia.org/>\n" + "PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\n"
			+ "PREFIX dbo: <http://dbpedia.org/ontology/>\n";

	/**
	 * A SPARQL endpoint which can be used to query the RDF repository.
	 */
	public static SparqlEndpoint endpoint;

	/**
	 * An executor to be used to submit asynchronous tasks which might be subjected
	 * to a time-out.
	 */
	public static ExecutorService executor;

	/**
	 * The output file in json
	 */
	public static FileWriter output;

	/**
	 * A service native method to query for CPU usage.
	 * <p>
	 * The name and implementation of this method are adapted from <a href=
	 * "http://www.javaworld.com/article/2077361/learn-java/profiling-cpu-usage-from-within-a-java-application.html">this
	 * 2002 blog post</a>.
	 * </p>
	 * <p>
	 * The implementation in C language of this native method is contained in the
	 * two source files <code>rdfminer_RDFMiner.h</code> and
	 * <code>rdfminer_RDFMiner.c</code>.
	 * </p>
	 * 
	 * @return the number of milliseconds of CPU time used by the current process so
	 *         far
	 */
	public static native long getProcessCPUTime();

	/**
	 * The entry point of the RDF Miner application.
	 */
	public static void main(String[] args) {

		// Configure the log4j loggers:
		PropertyConfigurator.configure(Global.LOG4J_PROPERTIES);

		// Parse the command-line parameters and options:
		CmdLineParser parser = new CmdLineParser(parameters);

		// if you have a wider console, you could increase the value;
		// here 80 is also the default
		parser.getProperties().withUsageWidth(80);

		try {
			// parse the arguments.
			parser.parseArgument(args);
		} catch (CmdLineException e) {
			// if there's a problem in the command line, you'll get this
			// exception. this will report an error message.
			System.err.println(e.getMessage());
			// print the list of available options
			System.err.println();
			parser.printUsage(System.err);
			System.err.println();
			return;
		}

		if (RDFMiner.parameters.help) {
			// print the list of available options
			System.out.println();
			parser.printUsage(System.out);
			System.out.println();
			return;
		}

		// Get environment variable from container (defined in Dockerfile)
		logger.info("This is RDF Miner, version " + System.getenv("RDFMINER_VERSION"));
		// Load rdfminer_RDFMINER.so generated by ./compile_c_code.sh (see /scripts
		// folder)
		System.loadLibrary("rdfminer_RDFMiner");
		// Set SPARQL Endpoit
		endpoint = new SparqlEndpoint(Global.SPARQL_ENDPOINT, PREFIXES);

		AxiomGenerator generator = null;
		BufferedReader axiomFile = null;

		// Create an empty JSON object which will be fill with our results
		JSONObject json = new JSONObject();

		if (parameters.axiomFile == null) {
			if (parameters.axiom == null) {
				if (parameters.useRandomAxiomGenerator) {
					logger.info(
							"Initializing the random axiom generator with grammar " + parameters.grammarFile + "...");
					generator = new RandomAxiomGenerator(parameters.grammarFile);
				} else if (parameters.subclassList != null) {
					logger.info("Initializing the increasing TP axiom generator...");
					generator = new IncreasingTimePredictorAxiomGenerator(parameters.subclassList);
				} else {
					logger.info("Initializing the candidate axiom generator...");
					generator = new CandidateAxiomGenerator(parameters.grammarFile);
				}
			} else {
				logger.info("launch test on a single axiom");
			}
		} else {
			logger.info("Reading axioms from file " + parameters.axiomFile + "...");
			try {
				// Try to read the status file:
				axiomFile = new BufferedReader(new FileReader(parameters.axiomFile));
			} catch (IOException e) {
				logger.error("Could not open file " + parameters.axiomFile);
				return;
			}
		}

		executor = Executors.newSingleThreadExecutor();

		if (parameters.axiom == null) {
			// as the test of a single axiom is return on standard output, we don't need to
			// write
			// file of the results
			try {
				output = new FileWriter(Global.OUTPUT_PATH + parameters.resultFile + ".json");
			} catch (IOException e) {
				logger.error(e.getMessage());
				e.printStackTrace();
				System.exit(1);
			}
		}

		while (true) {

			Axiom a = null;
			String axiomName = null;
			long t0 = getProcessCPUTime();

			if (generator != null) {
				Phenotype axiom = generator.nextAxiom();
				if (axiom == null)
					break;
				axiomName = axiom.getStringNoSpace();
				logger.info("Testing axiom: " + axiomName);
				try {
					a = AxiomFactory.create(axiom);
				} catch (QueryExceptionHTTP httpError) {
					logger.error("HTTP Error " + httpError.getMessage() + " making a SPARQL query.");
					httpError.printStackTrace();
					System.exit(1);
				} catch (JenaException jenaException) {
					logger.error("Jena Exception " + jenaException.getMessage() + " making a SPARQL query.");
					jenaException.printStackTrace();
					System.exit(1);
				}
			} else {
				try {
					if (axiomFile == null && parameters.axiom != null) {
						axiomName = parameters.axiom;
					} else if (axiomFile != null && parameters.axiom == null) {
						axiomName = axiomFile.readLine();
					} else {
						logger.error("'-a' and '-sa' are used at the same time");
						System.exit(1);
					}
					if (axiomName == null)
						break;
					if (axiomName.isEmpty())
						break;
					logger.info("Testing axiom: " + axiomName);
					a = AxiomFactory.create(axiomName);
				} catch (IOException e) {
					logger.error("Could not read the next axiom.");
					e.printStackTrace();
					System.exit(1);
				}
			}

			// long t = System.currentTimeMillis();
			long t = getProcessCPUTime();

			if (a != null) {
				// Save a JSON report of the test
				AxiomTestJSON reportJSON = new AxiomTestJSON();
				reportJSON.elapsedTime = t - t0;
				reportJSON.referenceCardinality = a.referenceCardinality;
				reportJSON.numConfirmations = a.numConfirmations;
				reportJSON.numExceptions = a.numExceptions;
				reportJSON.possibility = a.possibility().doubleValue();
				reportJSON.necessity = a.necessity().doubleValue();
				reportJSON.isTimeout = a.isTimeout;
				if (a.numConfirmations > 0 && a.numConfirmations < 100)
					reportJSON.confirmations = a.confirmations;
				if (a.numExceptions > 0 && a.numExceptions < 100)
					reportJSON.exceptions = a.exceptions;
				
				// fill json results
				json.append(axiomName, reportJSON.toJSON());

				// print useful results
				logger.info("Num. confirmations: " + a.numConfirmations);
				logger.info("Num. exceptions: " + a.numExceptions);
				logger.info("Possibility = " + a.possibility().doubleValue());
				logger.info("Necessity = " + a.necessity().doubleValue());

				if (a instanceof SubClassOfAxiom && a.necessity().doubleValue() > 1.0 / 3.0) {
					SubClassOfAxiom sa = (SubClassOfAxiom) a;
					SubClassOfAxiom.maxTestTime.maxput(sa.timePredictor(), t - t0);
				}

				if(parameters.axiom != null) {
					System.out.println("[RES]" + json.toString());
					break;
				}
				
			} else
				logger.warn("Axiom type not supported yet!");
			logger.info("Test completed in " + (t - t0) + " ms.");
		}
		logger.info("Done testing axioms. Exiting.");
		if (parameters.axiom == null) {
			try {
				output.write(json.toString());
				output.close();
			} catch (IOException e) {
				logger.error("I/O error while closing CSV writer: " + e.getMessage());
				e.printStackTrace();
				System.exit(1);
			}
		}
		System.exit(0);
	}

}
